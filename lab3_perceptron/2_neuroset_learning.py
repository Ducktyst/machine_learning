import numpy as np

"""
227
"""
# Определяем функцию активации
def activation(x):
    return 1 / (1 + np.exp(-x))


# Определяем производную от функции активации
def sigma_derivative(x):
    return x * (1 - x)


# X - это матрица, где столбцы это признаки,
# а строки это объекты выборки.
X = np.array([[0, 0, 1],
              [0, 1, 0],
              [1, 0, 0],
              [1, 1, 1]])
# Y - это матрица, где столбцы это признаки
# (в данном случае один целевой признак),
# а строки это объекты выборки.
y = np.array([[1],
              [1],
              [1],
              [0]])

# Зафиксируем генератор случайных чисел, чтобы получать каждый раз
# предсказуемый (одинаковый) результат
np.random.seed(4)

# Нотация слоев - L1, L2, L3.
# В каждом слое есть n1, n2, n3 нейронов.
# Строки в матрицах индексируются как i, столбцы как j.
# W_1_2 - это матрица весов между первым и вторым слоем.
# Строки определяют левый нейрон (т.е. нейрон первого слоя) и соответственно
# количество строк равно n1.

# Столбцы определяют правый нейрон (т.е. нейрон второго слоя) и соответственно
# количество столбцов равно n2.
# На пересечении i-й строки и j-го столбца получаем ячейку с конкретным весом,
# который связывает i-ый левый нейрон (слоя L1) и j-й правый нейрон (слоя L2)
w12 =  np.random.random((3, 2))
double_12 = 2 * w12
result = double_12 - 1

W_1_2 = 2 * np.random.random((3, 2)) - 1
# W_2_3 - это матрица весов между вторым и третьим слоем. Все остальное как в W_1_2
W_2_3 = 2 * np.random.random((2, 1)) - 1
# скорость движения по антиградиенту
speed = 1.1
# цикл прогона модели
for j in range(100000):
    # lo, l1, l2 - матрицы определенного слоя сети. Каждая строка матрицы это реакция
    # на i-й объект входа. Каждая колонка матрицы это реакция j-го нейрона
    # соответствующего слоя на разные входные образы.
    # На пересечении i-й строки и j-го столбца получаем ячейку с конкретной
    # реакцией j-го нейрона на конкретный вход.
    # Первый слой нейронов полностю принимает значения входа Х (т.е. все реакции
    # единичные).
    l1 = X
    # Второй слой нейронов рассчитывается как функция активации по каждому
    # элементу матрицы U
    # По сути l2 это матрица 4 на 4, где в каждой ячейке результат активации нейрона
    # второго слоя для всех 4-х входных образов
    # Детально:
    # матрица U = np.dot(l0, W_0_1) является результатом
    # матричного произведения выходов нейронов предыдущего слоя на веса между 1
    # и 2 слоем.
    # Строки матрицы U отвечают за конкретный входной образ (объект).
    # Столбцы матрицы U отвечают за нейроны правого слоя (L2).
    # На пересечении i-й строки и j-го столбца получаем ячейку с конкретной
    # взвешенной суммой
    # для i-го входного образа и j-го нейрона слоя (l2). Иными словами, для каждого
    # нейрона слоя L2 и для каждого входа
    # считается U = W1X1 + W2X2 + W3X3 (поскольку входной нейрон содержит 3
    # нейрона).
    l2 = activation(np.dot(l1, W_1_2))
    # тоже самое проделываем для Третьего слоя

    # По сути l3 это матрица 4 на 1, где в каждой ячейке результат активации нейрона
    # третьего слоя (а он один)
    # для всех 4-х входных образов
    l3 = activation(np.dot(l2, W_2_3))
    # рассчитывает ошибку на выходе
    l3_error = y - l3
    # рассчитывает модуль средней ошибки
    if (j % 10000) == 0:
        print("Error:" + str(np.mean(np.abs(l3_error))))

    # sigma - есть локальный градиент ошибки
    # l3_sigma рассчитывается как ошибка выхода всей сети на производную функции
    # активации всех нейронов L3.
    # На пересечении i-й строки и j-го столбца получаем ячейку с конкретной сигмой
    # для i-го входного образа и j-го нейрона слоя (l3). Иными словами, для каждого
    # нейрона слоя L3
    # и для каждого входа рассчитывается производная по функции активации
    # умноженная на ошибку.
    # То есть l3_sigma будет матрица 4 на 1 (поскольку в L3 только один нейрон).
    l3_sigma = l3_error * sigma_derivative(l3)
    print(l3_sigma)
    # Ошибка L2 слоя оценивается через взвешенную сигму слоя L3 по весам между
    # L2 и L3
    # Поскольку l3_sigma это матрица 4 на 1 и матрица W_2_3 4 на 1, то последнюю
    # матрицу
    # надо Транспонировать, чтобы выполнить правило умножения матриц и взвесить
    # элементы l3_sigma
    # по элементам W_2_3.
    # Тогда итоговая матрица l1_error будет 4 на 4, где на пересечении i-й строки и j-го
    # столбца
    # будет ячейка с конкретной ошибкой j-го нейрона слоя L2 для i-го входного
    # образа.
    l2_error = l3_sigma.dot(W_2_3.T)
    print(l2_error)
    # l2_sigma рассчитывается как ошибка слоя L2 на производную функции активации
    # всех нейронов L2. На пересечении i-й строки и j-го столбца получаем ячейку с
    # конкретной сигмой для i-го входного образа и j-го нейрона слоя (L2). Иными
    # словами, для каждого нейрона слоя L2 и для каждого входа рассчитывается
    # производная по функции активации, умноженная на ошибку.

    # То есть l2_sigma будет матрица 4 на 4 (поскольку в L2 четыре нейрона).
    l2_sigma = l2_error * sigma_derivative(l2)
    print(l2_sigma)
    # обновляем веса
    # l2 это матрица 4 на 4, где в каждой ячейке результат активации нейрона второго
    # слоя для всех 4-х входных образов (по строкам).
    # А l3_sigma это матрица 4 на 1, где в каждой строке локальный градиент ошибки
    # для 4-х входных образов.
    # Чтобы взвесить столбец матрицы l2 по локальному градиенту (l3_sigma), нужно
    # транспонировать матрицу l2, чтобы
    # результат активации каждого нейрона в слое L2 по каждому входному образу
    # вытянулся в строку.
    # Тогда соответствующая строка умножится на столбик l3_sigma.
    # Заметьте, что транспонирование l3_sigma не даст результата, так как тогда в
    # каждом столбце
    # l3_sigma будет по одному значению, а в каждой строке l2 по 4 значения, а значит
    # такое перемножение матриц не пройдет. Применяем транспонирование к l2.
    W_2_3 += speed * l2.T.dot(l3_sigma)
    # аналогично W_2_3
    W_1_2 += speed * l1.T.dot(l2_sigma)
# Прямое распространение для тестовых данных
X_test = np.array([[0, 0, 0],
                   [0, 1, 1],
                   [1, 0, 1],
                   [1, 1, 0],
                   [0.5, 0.5, 0],
                   [0.5, 0.5, 1]])
# Y_test должен получиться [1, 0, 0, 1, 1, 0]
l1 = X_test
l2 = activation(np.dot(l1, W_1_2))
l3 = activation(np.dot(l2, W_2_3))
print(l3)

"""
http://www.machinelearning.ru/wiki/index.php?title=%D0%9E%D0%B4%D0%BD%D0%BE%D1%81%D0%BB%D0%BE%D0%B9%D0%BD%D1%8B%D0%B9_%D0%BF%D0%B5%D1%80%D1%81%D0%B5%D0%BF%D1%82%D1%80%D0%BE%D0%BD
"""